library("Rfacebook")
app_id = "1219535758094887"
app_secret = "0d7b93fb337020b9220de47a02b55d70"
fbOAuth(app_id = app_id, app_secret = app_secret)
library("Rfacebook")
app_id = "1219535758094887"
app_secret = "0d7b93fb337020b9220de47a02b55d70"
fbOAuth(app_id = app_id, app_secret = app_secret)
?fbOAuth
fbOAuth(app_id = app_id, app_secret = app_secret)
app_id = "1219535758094887"
app_secret = "0d7b93fb337020b9220de47a02b55d70"
fbOAuth(app_id = "1219535758094887", app_secret = "0d7b93fb337020b9220de47a02b55d70")
library("Rfacebook")
app_id = "1219535758094887"
app_secret = "0d7b93fb337020b9220de47a02b55d70"
token_id = "EAACEdEose0cBAO9nAAyUYC1ZCdyZBXEZCUOv61uiQvQT6KgZBEoqmkHsRGs4QPYVFjqcvyvY7Yf0q1wmpUd5lJbPcD7u26v06G0uZCJHXGkBaJ9MJlpsibTNZC0NwB614qmqpRs1H6kBCN6Pn0a8nzfxIg4bwZBAKx1VI7QVNJOpZBB0kSBrH54x3CC34bXOi5cZD"
token = "EAACEdEose0cBADLNztcCQAzgjYRFWY0KxuF3xZAcn3vnhb6kkPuzpbB330aSydEpZBFz3xI1Nx2cZBlfG83XZCZANSaZAZAutPpxTKHWzYGCxQKvzZC5ooRaclb5nRwH46LZCn7omovUYekE8VLaEcIGcffaXJyfmawzz4UDRrDp5FWO10o5Q5tjSed2Sv345GzcZD"
me <- getUsers("me", token=token)
me
names(me)
me$name
my_friends <- getFriends(token, simplify=TRUE)
?getFriends
friends
my_friends
my_friends$name
my_friends_info <- getUsers(my_friends$id, token=token, private_info=TRUE)
my_friends_info
names(my_friends_info)
my_friends_info$location
cbind(my_friends_info$name,my_friends_info$location)
ssa_resident <- subset(my_friends_info$first_name,my_friends_info$location == "Salvador, Brazil")
ssa_resident
table(my_friends_info$relationship_status)
my_likes <- getLikes(user = "me", token = token)
my_likes
names(my_likes)
my_likes$names
view(my_friends)
view(my_likes)
?getPage
page_data = getPage(facebook_page_id, token = token, n= 10)
facebook_page_id = "484059694944787"
page_data = getPage(facebook_page_id, token = token, n= 10)
page_data
names(page_data)
page_data$from_name
View(page_data)
group_name = "DataMining"
data_mining_groups = searchGroup(group_name, token = token)
data_mining_groups
View(data_mining_groups)
group_name = "GovernoBahia"
governo_bahia_groups = searchGroup(group_name, token = token)
governo_bahia_groups
View(governo_bahia_groups)
id_governo_bahia = "230798307122484"
governo_bahia_page_data = getPage(id_governo_bahia, token = token, n= 50)
id_rasta = "484059694944787"
rasta_page_data = getPage(id_rasta, token = token, n= 10)
View(governo_bahia_groups)
View(data_mining_groups)
?getGroup
id_group_data_mining = "161422081743"
group_posts <- getGroup (id_group_data_mining, token = token, n = 50)
View(data_mining_groups)
View(group_posts)
View(group_posts)
View(governo_bahia_page_data)
View(governo_bahia_groups)
View(data_mining_groups)
View(group_posts)
library("Rfacebook")
app_id = "1219535758094887"
app_secret = "0d7b93fb337020b9220de47a02b55d70"
token = "EAACEdEose0cBADLNztcCQAzgjYRFWY0KxuF3xZAcn3vnhb6kkPuzpbB330aSydEpZBFz3xI1Nx2cZBlfG83XZCZANSaZAZAutPpxTKHWzYGCxQKvzZC5ooRaclb5nRwH46LZCn7omovUYekE8VLaEcIGcffaXJyfmawzz4UDRrDp5FWO10o5Q5tjSed2Sv345GzcZD"
me <- getUsers("me", token=token)
me <- getUsers("me", token=token)
token = "EAACEdEose0cBANI2hoJb7xodEgWCJNfez8egzZBAvjtgFjl5mCA6NzUal4P3i5pogQ1OiBnJijzyZCrHG6baXyg7bLNEPYZB43h6PPFZBF6hBlgZC4KuEjEN0YHfe2SfeKQnDHe7rtnf2tIJ9ZAPpramgRKHZCwHygaGcVjI9cNhhm245TZARC7TidDr6whoqegZD"
me <- getUsers("me", token=token)
?getFriends
id_rasta = "484059694944787"
rasta_page_data = getPage(id_rasta, token = token, n= 10)
View(rasta_page_data)
?getPage
rasta_page_data = getPage(id_rasta, token = token, feed=TRUE, n= 1000)
View(rasta_page_data)
unique(sort(rasta_page_data$from_id))
length(sort(rasta_page_data$from_id))
unique(sort(rasta_page_data$id))
nnodes <- length(unique(sort(rasta_page_data$id)))
nnodes
nnodes <- length(sort(rasta_page_data$id))
nnodes
nodes <- sort(rasta_page_data$id)
nnodes <- length(rasta_page_data$id)
# each post is a node of the network
nodes <- rasta_page_data$id
#author of post
author <- rasta_page_data$from_id
i<-1
rasta_page_data$from_id[i]
which(rasta_page_data$from_id[i] %in% rasta_page_data$from_id)
rasta_page_data$from_id %in% rasta_page_data$from_id[i]
connections_i <- which(rasta_page_data$from_id %in% rasta_page_data$from_id[i])
cat(paste(i,connections_i,sep=" "),sep="\n")
connections_i <- which(author %in% author[i])
connections_i <- which(author %in% author[i])
length(connections_i)
nodes[i]
cat(paste(author[i],author[connections_i],sep=" "),sep="\n")
cbind(author[i],author[connections_i])
author
author[i]
cbind(nodes[i],nodes[connections_i])
nodes[i]
nodes[connections_i]
nodes[connections_i]
nodes[i]
cbind(nodes[i],nodes[connections_i])
cat(nodes[i],nodes[connections_i])
cat(paste(nodes[i],nodes[connections_i],sep=" "),sep="\n")
likes <- rasta_page_data$likes_count
cat(paste(nodes[i],nodes[connections_i],likes[i],sep=" "),sep="\n")
cat(paste("*Vertices",nnodes,"\n",sep=""))
cat(paste("*Vertices ",nnodes,"\n",sep=""))
for(i in 1:nnodes){
cat(paste(nodes[i],,likes[i],sep=" "),sep="\n")
}
for(i in 1:nnodes){
cat(paste(nodes[i],likes[i],sep=" "),sep="\n")
}
cat("*Edges")
for(i in 1:nnodes){
connections_i <- which(author %in% author[i])
cat(paste(nodes[i],nodes[connections_i],sep=" "),sep="\n")
}
filename <- "posts_network.net"
cat(paste("*Vertices ",nnodes,"\n",sep=""),file=filename)
for(i in 1:nnodes){
cat(paste(nodes[i],likes[i],sep=" "),sep="\n",file=filename,append=TRUE)
}
cat("*Edges",file=filename,append=TRUE)
for(i in 1:nnodes){
connections_i <- which(author %in% author[i])
cat(paste(nodes[i],nodes[connections_i],sep=" "),sep="\n",file=filename,append=TRUE)
}
getUsers(id_rasta, token)
getUsers("me", token)
getPage(id_rasta,token = token)
keyword = "#GovernoDaBahia"
status_governo_bahia = searchFacebook(keyword, token = token, n = 2000, "yesterday 00:00", until = "yesterday 23:59")
my_friends <- getFriends(token, simplify = TRUE)
?getFriends
getNetwork(token, format = "adj.matrix")
?getNetwork
library("Rfacebook")
app_id = "1219535758094887"
app_secret = "0d7b93fb337020b9220de47a02b55d70"
token = "EAACEdEose0cBAI982I1SpESjEcDUB2ulb7PciGEyejHYP4ZCuOrHOuRrBorcASGKKB4jRB088ddCNxalXiox9MEE6KvD0lj92tjby27ZB2DteLseOssgjZCkFYLCZB1ZCmuENZArxx6wLEULdYS6InXvQQ9AYNtT6wamqRocAd1AH1OyrId5ZBhvOGaT3XzCAcZD"
id_post = "484059694944787_1468376633179750"
post = getPost(id_post, token, n = 500)
post
names(post)
post$post
post$comments
View(rasta_page_data)
View(group_posts)
View(page_data)
id_post = "484059694944787_1430601980290549"
post = getPost(id_post, token, n = 500)
names(post)
post$comments
post
str(post)
post
post$from_id
post$comments$from_id
post$comments
post$comments
names(post)
post$likes
post$comments
post$comments$message
post$comments$likes_count
post$comments$created_time
post$comments$from_id
getUsers(post$comments$from_id[1],token=token)
?getUsers
library("Rfacebook")
app_id = "1219535758094887"
app_secret = "0d7b93fb337020b9220de47a02b55d70"
token = "EAACEdEose0cBAI982I1SpESjEcDUB2ulb7PciGEyejHYP4ZCuOrHOuRrBorcASGKKB4jRB088ddCNxalXiox9MEE6KvD0lj92tjby27ZB2DteLseOssgjZCkFYLCZB1ZCmuENZArxx6wLEULdYS6InXvQQ9AYNtT6wamqRocAd1AH1OyrId5ZBhvOGaT3XzCAcZD"
id_rasta = "484059694944787"
rasta_page_data = getPage(id_rasta, token = token, feed=TRUE, n= 100)
id_post = "484059694944787_1430601980290549"
rasta_page_data$comments_count
which.max(rasta_page_data$comments_count)
rasta_page_data$id
id_post = rasta_page_data$id[57]
id_post
post = getPost(id_post, token, n = 500)
post = getPost(id_post, token)
source("textPreproc.R")
source("sentimentAnalysis.R")
source("auxFunctions.R")
source("twitterMining.R")
library("wordcloud")
library("maps")
library("mapdata")
library("rworldmap")
library("maptools")
install.packages("rgeos")
source("textPreproc.R")
source("sentimentAnalysis.R")
source("auxFunctions.R")
source("twitterMining.R")
library("wordcloud")
library("maps")
library("mapdata")
library("rworldmap")
library("maptools")
setwd("/home/cdesantana/TwitterAnalytics/ObjectivaR")
source("textPreproc.R")
source("sentimentAnalysis.R")
source("auxFunctions.R")
source("twitterMining.R")
library("wordcloud")
library("maps")
library("mapdata")
library("rworldmap")
library("maptools")
connectTwitterApp()
textToSearch <- "@costa_rui"
xlimits<-c(-46.600952, -37.3409874)
ylimits<-c(-18.328010, -8.5409874)
noOfTweets <- 10000
0
some_tweets = searchTwitter(textToSearch, n=noOfTweets, lang='pt-br')
some_tweets
df_tweets <- twListToDF(some_tweets)
map("world","Brazil")
points(df_tweets$longitude,df_tweets$latitude)
df_tweets$latitude
names(df_tweets)
df_tweets$favoriteCount
hist(df_tweets$favoriteCount)
hist(df_tweets$favoriteCount,nclass = 20)
hist(df_tweets$favoriteCount,nclass = 210)
hist(df_tweets$favoriteCount,nclass = 10)
names(df_tweets)
df_tweets$id
names(df_tweets)
df_tweets$replyToUID
names(df_tweets)
df_tweets$isRetweet
names(df_tweets)
df_tweets$retweetCount
hist(df_tweets$retweetCount)
some_txt = sapply(some_tweets, function(x) x$getText())
some_txt
length(some_txt)
some_tweets[1]
as.character(some_tweets[1])
as.character(unlist(some_tweets[1]))
as.character(unlist(some_tweets[[1]]))
some_tweets[[1]]
library(tm)
write.csv(some_tweets,file="./ruicosta_tweets.csv",row.names = F)
write.csv(df_tweets,file="./ruicosta_tweets.csv",row.names = F)
library(wordcloud)
library(tm)
cname <- "/home/cdesantana/TwitterAnalytics/ObjectivaR/texts"
some_txt
some_txt[[]]
some_txt[]
unlist(some_txt[])
unlist(some_txt)
as.character(unlist(some_txt))
length(as.character(unlist(some_txt)))
as.character(unlist(some_txt))[1]
length(as.character(unlist(some_txt)))
as.character(unlist(some_txt))
write.csv(as.character(unlist(some_txt)),file="./plaintext_ruicosta.txt")
write.csv(as.character(unlist(some_txt)),file="./plaintext_ruicosta.txt",row.names = FALSE)
write.csv(as.character(unlist(some_txt)),file="./plaintext_ruicosta.txt",row.names = FALSE,quote = NULL)
write.csv(as.character(unlist(some_txt)),file="./plaintext_ruicosta.txt",row.names = FALSE,quote = )
cname <- "/home/cdesantana/TwitterAnalytics/ObjectivaR/texts_rui"
docs <- Corpus(DirSource(cname))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, PlainTextDocument)
dtm <- TermDocumentMatrix(docs)
tdm <- TermDocumentMatrix(docs)
freq <- colSums(as.matrix(dtm))
m1 <- as.matrix(tdm)
freq <- colSums(as.matrix(tdm))
d1 <- data.frame(word = names(v1), freq = v1)
v1 <- sort(rowSums(m1), decreasing = TRUE)
d1 <- data.frame(word = names(v1), freq = v1)
wordcloud(d1$word, d1$freq, col = brewer.pal(8, "set2"), min.freq = 1)
wordcloud(d1$word, d1$freq, col = brewer.pal(8, "Set2"), min.freq = 1)
wordcloud(d1$word, d1$freq, col = brewer.pal(8, "Set2"), min.freq = 10)
hist(d1$freq)
summary(d1$freq)
d1$word[order(d1$freq)]
d1$word[order(d1$freq)][1:10]
d1$freq
order(d1$freq)
order(d1$freq)[1000:1585]
sort(d1$freq)
sort(d1$freq)[1:1585]
sort(d1$freq)[1000:1585]
max(d1$freq)
length(d1$freq)
sort(d1$freq)[2000:2453]
d1$word[order(d1$freq)][2000:2453]
sort(d1$freq)[2000:2453]
d1$word[order(d1$freq)][2000:2453]
sort(d1$freq)[2000:2453]
wordcloud(d1$word, d1$freq, col = brewer.pal(8, "Set2"), min.freq = 20)
d1$word[order(d1$freq)]
d1$word[order(d1$freq)][2400:2453]
docs <- Corpus(DirSource(cname))
docs <- tm_map(docs, removeWords, c("httpstcoxeoz","pra","como","dos","uma","por","das"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, PlainTextDocument)
tdm <- TermDocumentMatrix(docs)
m1 <- as.matrix(tdm)
freq <- colSums(as.matrix(tdm))
v1 <- sort(rowSums(m1), decreasing = TRUE)
d1 <- data.frame(word = names(v1), freq = v1)
wordcloud(d1$word, d1$freq, col = brewer.pal(8, "Set2"), min.freq = 20)
d1$freq
sort(d1$freq)[2400:2453]
wordcloud(d1$word, d1$freq, col = brewer.pal(8, "Set2"), min.freq = 50)
cname <- "/home/cdesantana/TwitterAnalytics/ObjectivaR/texts_rui"
docs <- Corpus(DirSource(cname))
docs <- tm_map(docs, removeWords, c("httpstcoxeoz","pra","como","dos","uma","por","das","httpstcohinafl"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, PlainTextDocument)
tdm <- TermDocumentMatrix(docs)
m1 <- as.matrix(tdm)
freq <- colSums(as.matrix(tdm))
v1 <- sort(rowSums(m1), decreasing = TRUE)
d1 <- data.frame(word = names(v1), freq = v1)
wordcloud(d1$word, d1$freq, col = brewer.pal(8, "Set2"), min.freq = 50)
df_tweets$replyToSID
df_tweets$isRetweet
sum(df_tweets$isRetweet)
length(df_tweets$isRetweet)
df_tweets$created
range(df_tweets$created)
docs <- Corpus(DirSource(cname))
docs <- tm_map(docs, removeWords, c("httpstcoxeoz","pra","como","dos","uma","por","das","httpstcohinafl","costarui"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, PlainTextDocument)
tdm <- TermDocumentMatrix(docs)
m1 <- as.matrix(tdm)
freq <- colSums(as.matrix(tdm))
v1 <- sort(rowSums(m1), decreasing = TRUE)
d1 <- data.frame(word = names(v1), freq = v1)
wordcloud(d1$word, d1$freq, col = brewer.pal(8, "Set2"), min.freq = 50)
docs <- Corpus(DirSource(cname))
docs <- tm_map(docs, removeWords, c("httpstcoxeoz","pra","como","dos","uma","por","das","httpstcohinafl","costarui","que"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, PlainTextDocument)
tdm <- TermDocumentMatrix(docs)
m1 <- as.matrix(tdm)
freq <- colSums(as.matrix(tdm))
v1 <- sort(rowSums(m1), decreasing = TRUE)
d1 <- data.frame(word = names(v1), freq = v1)
wordcloud(d1$word, d1$freq, col = brewer.pal(8, "Set2"), min.freq = 50)
docs <- Corpus(DirSource(cname))
docs <- tm_map(docs, removeWords, c("httpstcoxeoz","pra","como","dos","uma","por","das","httpstcohinafl","costarui","que","para"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, PlainTextDocument)
tdm <- TermDocumentMatrix(docs)
m1 <- as.matrix(tdm)
freq <- colSums(as.matrix(tdm))
v1 <- sort(rowSums(m1), decreasing = TRUE)
d1 <- data.frame(word = names(v1), freq = v1)
wordcloud(d1$word, d1$freq, col = brewer.pal(8, "Set2"), min.freq = 50)
